{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Corpora\n",
    "\n",
    "Now that we have looked at analyzing and comparing documents, we can move to a higher unit of text. Sometime we want to look at a large collection of text in aggregate, such as the complete works of William Shakespeare, or all New York Times articles ever. The term we use for a collection of documents is corpus. And a corpus can be as large or as small as you want, but are usually collected together for some reason and have some meaning behind why they are grouped together. \n",
    "\n",
    "Lets look at a few examples we have direct access to through NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Preamble\n",
    "#\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import our core libraries\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\"1789-Washington.txt\",\n",
    "\"1865-Lincoln.txt\",\n",
    "\"1941-Roosevelt.txt\",\n",
    "\"1793-Washington.txt\",\n",
    "\"1869-Grant.txt\",\n",
    "\"1945-Roosevelt.txt\",\n",
    "\"1797-Adams.txt\",\n",
    "\"1873-Grant.txt\",\n",
    "\"1949-Truman.txt\",\n",
    "\"1801-Jefferson.txt\",\n",
    "\"1877-Hayes.txt\",\n",
    "\"1953-Eisenhower.txt\",\n",
    "\"1805-Jefferson.txt\",\n",
    "\"1881-Garfield.txt\",\n",
    "\"1957-Eisenhower.txt\",\n",
    "\"1809-Madison.txt\",\n",
    "\"1885-Cleveland.txt\",\n",
    "\"1961-Kennedy.txt\",\n",
    "\"1813-Madison.txt\",\n",
    "\"1889-Harrison.txt\",\n",
    "\"1965-Johnson.txt\",\n",
    "\"1817-Monroe.txt\",\n",
    "\"1893-Cleveland.txt\",\n",
    "\"1969-Nixon.txt\",\n",
    "\"1821-Monroe.txt\",\n",
    "\"1897-McKinley.txt\",\n",
    "\"1973-Nixon.txt\",\n",
    "\"1825-Adams.txt\",\n",
    "\"1901-McKinley.txt\",\n",
    "\"1977-Carter.txt\",\n",
    "\"1829-Jackson.txt\",\n",
    "\"1905-Roosevelt.txt\",\n",
    "\"1981-Reagan.txt\",\n",
    "\"1833-Jackson.txt\",\n",
    "\"1909-Taft.txt\",\n",
    "\"1985-Reagan.txt\",\n",
    "\"1837-VanBuren.txt\",\n",
    "\"1913-Wilson.txt\",\n",
    "\"1989-Bush.txt\",\n",
    "\"1841-Harrison.txt\",\n",
    "\"1917-Wilson.txt\",\n",
    "\"1993-Clinton.txt\",\n",
    "\"1845-Polk.txt\",\n",
    "\"1921-Harding.txt\",\n",
    "\"1997-Clinton.txt\",\n",
    "\"1849-Taylor.txt\",\n",
    "\"1925-Coolidge.txt\",\n",
    "\"2001-Bush.txt\",\n",
    "\"1853-Pierce.txt\",\n",
    "\"1929-Hoover.txt\",\n",
    "\"2005-Bush.txt\",\n",
    "\"1857-Buchanan.txt\",\n",
    "\"1933-Roosevelt.txt\",\n",
    "\"2009-Obama.txt\",\n",
    "\"1861-Lincoln.txt\",\n",
    "\"1937-Roosevelt.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of \n"
     ]
    }
   ],
   "source": [
    "#filename = \"data/inaugural/1789-Washington.txt\"\n",
    "\n",
    "text_dict = {}\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(\"data/inaugural/\" + filename) as handle:\n",
    "        name = filename.replace(\".txt\", \"\")\n",
    "        text = handle.read()\n",
    "        text_dict[name] = text\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(text_dict[\"1789-Washington\"][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 2418), (u'the', 1616), (u\"'\", 1127), (u'.', 974), (u'and', 810), (u'to', 720), (u'a', 620), (u'she', 544), (u'it', 539), (u'of', 499), (u'said', 462), (u'!', 450), (u'alice', 396), (u'was', 366), (u'i', 364), (u'in', 359), (u'you', 356), (u'that', 284), (u'--', 264), (u'as', 256), (u'her', 248), (u':', 233), (u'at', 209), (u\"n't\", 204), (u'?', 202), (u\"'s\", 194), (u';', 194), (u'on', 191), (u'had', 184), (u'with', 179), (u'all', 178), (u\"'i\", 169), (u'be', 148), (u'for', 146), (u'so', 144), (u'very', 139), (u'they', 135), (u'not', 135), (u'this', 131), (u'but', 131), (u'little', 128), (u'do', 125), (u'he', 117), (u'is', 113), (u'out', 113), (u'what', 103), (u'down', 102), (u'one', 99), (u'up', 97), (u'his', 95)]\n"
     ]
    }
   ],
   "source": [
    "# Lets turn the text of alice in wonderland into a bag of words with\n",
    "# associated frequency distribution.\n",
    "\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "alice = nltk.word_tokenize(alice)\n",
    "alice = [word.lower() for word in alice]\n",
    "frequencies = nltk.FreqDist(alice)\n",
    "print(frequencies.most_common(50))\n",
    "\n",
    "# FreqDist docs.\n",
    "# http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Letâ€™s create a vector space model out of some documents\n",
    "#and calculate some pairwise similarities.\n",
    "\n",
    "# Extract and normalize tokens for a given document.\n",
    "def get_tokens(fileid, corpus):\n",
    "    raw = corpus.raw(fileid)\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    norm = [token.lower() for token in tokens]\n",
    "    return norm\n",
    "\n",
    "# Takes all the tokens that appear in token_lists and puts them in a \n",
    "# set to determine unique tokens. Then creates a dictionary mapping a token\n",
    "# to its index in the set, this enables us to have a unique target position for\n",
    "# each word in our vocabulary.\n",
    "def build_vocabulary(token_lists):\n",
    "    result = set()\n",
    "    for tl in token_lists:\n",
    "        result = result.union(set(tl))\n",
    "    result = {v:i for i,v in enumerate(result)}\n",
    "    return result\n",
    "\n",
    "# Builds a vector for a given token list and vocabulary.\n",
    "# The result is a vector with length equal to the number of words in the\n",
    "# vocabulary, and term weights for each token in the token list set in \n",
    "# the appropriate position\n",
    "def build_vector(tokens, vocabulary):\n",
    "    result = [0] * len(vocabulary)\n",
    "    freq = Counter(tokens)\n",
    "    for token in tokens:\n",
    "        pos = vocabulary[token]\n",
    "        result[pos] = freq[token]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24588\n"
     ]
    }
   ],
   "source": [
    "alice = get_tokens('carroll-alice.txt', gutenberg)\n",
    "moby = get_tokens('melville-moby_dick.txt', gutenberg)\n",
    "austen1 = get_tokens('austen-emma.txt', gutenberg)\n",
    "austen2 = get_tokens('austen-persuasion.txt', gutenberg)\n",
    "austen3 = get_tokens('austen-sense.txt', gutenberg)\n",
    "\n",
    "vocabulary = build_vocabulary([alice, moby, austen1, austen2, austen3]);\n",
    "print(len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_v = build_vector(alice, vocabulary)\n",
    "moby_v = build_vector(moby, vocabulary)\n",
    "austen1_v = build_vector(austen1, vocabulary)\n",
    "austen2_v = build_vector(austen2, vocabulary)\n",
    "austen3_v = build_vector(austen3, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 6, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "[u'gag', u'woods', u'clotted', u'pantheistic', u'hanging', u'woody', u'disobeying', u'canes', u'scold', u'stipulate', u'medicament', u'bringing', u'wooded', u'harville', u'wooden', u'wednesday', u'broiled', u'soladoes', u'crotch', u'sooty', u'insular', u'woollen-draper', u'miniatures', u'sooth', u'sustaining', u'consenting', u\"frigate's\", u'inanimate', u\"mind'em\", u'errors', u'semicircular', u'nature.', u'cooking', u'designing', u'shocks', u'crouch', u'work-bags', u'primogenitures', u'china', u'properest', u'natured', u'climbed', u'circumferences', u'natures', u'golden', u'_would_', u'projection', u'lengthen', u'hermaphroditical', u'stern']\n",
      "\n",
      "\n",
      "[(0, u'gag'), (0, u'woods'), (0, u'clotted'), (0, u'pantheistic'), (6, u'hanging'), (0, u'woody'), (0, u'disobeying'), (0, u'canes'), (0, u'scold'), (3, u'stipulate'), (2, u'medicament'), (0, u'bringing'), (0, u'wooded'), (0, u'harville'), (0, u'wooden'), (0, u'wednesday'), (0, u'broiled'), (0, u'soladoes'), (0, u'crotch'), (0, u'sooty'), (0, u'insular'), (0, u'woollen-draper'), (0, u'miniatures'), (0, u'sooth'), (0, u'sustaining'), (0, u'consenting'), (1, u\"frigate's\"), (0, u'inanimate'), (0, u\"mind'em\"), (0, u'errors'), (0, u'semicircular'), (1, u'nature.'), (0, u'cooking'), (0, u'designing'), (2, u'shocks'), (0, u'crouch'), (0, u'work-bags'), (0, u'primogenitures'), (0, u'china'), (0, u'properest'), (0, u'natured'), (0, u'climbed'), (17, u'circumferences'), (0, u'natures'), (0, u'golden'), (0, u'_would_'), (0, u'projection'), (0, u'lengthen'), (0, u'hermaphroditical'), (0, u'stern')]\n"
     ]
    }
   ],
   "source": [
    "print(alice_v[0:50])\n",
    "print(\"\\n\")\n",
    "\n",
    "#this code is slightly wrong - it was changed during the course to accurately match words to frequencies\n",
    "\n",
    "print(list(vocabulary)[0:50])\n",
    "print(\"\\n\")\n",
    "print(zip(alice_v[0:50], list(vocabulary)[0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.902451480786\n",
      "0.881000554468\n",
      "0.893427441523\n",
      "0.88611793731\n"
     ]
    }
   ],
   "source": [
    "# Lets compare Alice in Wonderland to the Others\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Clustering results:', [0, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Lets see this in action\n",
    "# http://www.nltk.org/api/nltk.cluster.html\n",
    "# http://www.nltk.org/_modules/nltk/cluster/kmeans.html\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from numpy import array\n",
    "\n",
    "num_clusters = 3\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=5)\n",
    "\n",
    "vectors = [array(f) for f in all_vectors] \n",
    "clusters = kclusterer.cluster(vectors, True) \n",
    "print('Clustering results:', clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets do this for all 18 documents\n",
    "\n",
    "doc_ids = gutenberg.fileids()\n",
    "token_lists = [get_tokens(f, gutenberg) for f in doc_ids]\n",
    "voc = build_vocabulary(token_lists)\n",
    "doc_vectors = [array(build_vector(tl, voc)) for tl in token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'austen-emma.txt', 9)\n",
      "(u'austen-persuasion.txt', 6)\n",
      "(u'austen-sense.txt', 9)\n",
      "(u'bible-kjv.txt', 11)\n",
      "(u'blake-poems.txt', 7)\n",
      "(u'bryant-stories.txt', 3)\n",
      "(u'burgess-busterbrown.txt', 0)\n",
      "(u'carroll-alice.txt', 7)\n",
      "(u'chesterton-ball.txt', 4)\n",
      "(u'chesterton-brown.txt', 5)\n",
      "(u'chesterton-thursday.txt', 4)\n",
      "(u'edgeworth-parents.txt', 10)\n",
      "(u'melville-moby_dick.txt', 7)\n",
      "(u'milton-paradise.txt', 8)\n",
      "(u'shakespeare-caesar.txt', 1)\n",
      "(u'shakespeare-hamlet.txt', 2)\n",
      "(u'shakespeare-macbeth.txt', 1)\n",
      "(u'whitman-leaves.txt', 8)\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 12 #note there are 12 authors\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "clusters = kclusterer.cluster(doc_vectors, True) \n",
    "\n",
    "doc_clusters = zip(gutenberg.fileids(), clusters)\n",
    "for dc in doc_clusters:\n",
    "    pprint(dc)\n",
    "    \n",
    "# Note that running this multiple times produces different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 9, 11, 7, 3, 0, 7, 4, 5, 4, 10, 7, 8, 1, 2, 1, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " array([ 0.5,  0. ,  0. , ...,  1. ,  0. ,  0. ]),\n",
       " array([ 0.,  4.,  0., ...,  1.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " array([ 0. ,  0. ,  0. , ...,  0. ,  0.5,  0. ]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.66666667,  0.        ]),\n",
       " array([ 0.5,  0. ,  0.5, ...,  0. ,  1.5,  0. ]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  3.])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clusters)\n",
    "kclusterer.means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1789-Washington.txt', u'1793-Washington.txt', u'1797-Adams.txt', u'1801-Jefferson.txt', u'1805-Jefferson.txt', u'1809-Madison.txt', u'1813-Madison.txt', u'1817-Monroe.txt', u'1821-Monroe.txt', u'1825-Adams.txt', u'1829-Jackson.txt', u'1833-Jackson.txt', u'1837-VanBuren.txt', u'1841-Harrison.txt', u'1845-Polk.txt', u'1849-Taylor.txt', u'1853-Pierce.txt', u'1857-Buchanan.txt', u'1861-Lincoln.txt', u'1865-Lincoln.txt', u'1869-Grant.txt', u'1873-Grant.txt', u'1877-Hayes.txt', u'1881-Garfield.txt', u'1885-Cleveland.txt', u'1889-Harrison.txt', u'1893-Cleveland.txt', u'1897-McKinley.txt', u'1901-McKinley.txt', u'1905-Roosevelt.txt', u'1909-Taft.txt', u'1913-Wilson.txt', u'1917-Wilson.txt', u'1921-Harding.txt', u'1925-Coolidge.txt', u'1929-Hoover.txt', u'1933-Roosevelt.txt', u'1937-Roosevelt.txt', u'1941-Roosevelt.txt', u'1945-Roosevelt.txt', u'1949-Truman.txt', u'1953-Eisenhower.txt', u'1957-Eisenhower.txt', u'1961-Kennedy.txt', u'1965-Johnson.txt', u'1969-Nixon.txt', u'1973-Nixon.txt', u'1977-Carter.txt', u'1981-Reagan.txt', u'1985-Reagan.txt', u'1989-Bush.txt', u'1993-Clinton.txt', u'1997-Clinton.txt', u'2001-Bush.txt', u'2005-Bush.txt', u'2009-Obama.txt']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.\n",
    "\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from numpy import array\n",
    "\n",
    "# Extract and normalize tokens for a given document.\n",
    "def get_tokens(fileid, corpus):\n",
    "    raw = corpus.raw(fileid)\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    norm = [token.lower() for token in tokens]\n",
    "    return norm\n",
    "\n",
    "# Takes all the tokens that appear in token_lists and puts them in a \n",
    "# set to determine unique tokens. Then creates a dictionary mapping a token\n",
    "# to its index in the set, this enables us to have a unique target position for\n",
    "# each word in our vocabulary.\n",
    "def build_vocabulary(token_lists):\n",
    "    result = set()\n",
    "    for tl in token_lists:\n",
    "        result = result.union(set(tl))\n",
    "    result = {v:i for i,v in enumerate(result)}\n",
    "    return result\n",
    "\n",
    "# Builds a vector for a given token list and vocabulary.\n",
    "# The result is a vector with length equal to the number of words in the\n",
    "# vocabulary, and term weights for each token in the token list set in \n",
    "# the appropriate position\n",
    "def build_vector(tokens, vocabulary):\n",
    "    result = [0] * len(vocabulary)\n",
    "    freq = Counter(tokens)\n",
    "    for token in tokens:\n",
    "        pos = vocabulary[token]\n",
    "        result[pos] = freq[token]\n",
    "    return result\n",
    "\n",
    "# The ids for the speeches. Print these out if you want to get a sense of\n",
    "# what is in this corpus.\n",
    "speech_names = inaugural.fileids()\n",
    "print speech_names\n",
    "\n",
    "# Step 1. Tokenize the speeches\n",
    "token_lists = [get_tokens(f, inaugural) for f in speech_names]\n",
    "\n",
    "# Step 2. Build a vocabulary for all the speeches this is the\n",
    "voc = build_vocabulary(token_lists)\n",
    "\n",
    "\n",
    "# Step 3. Build feature vectors for the individual speeches.  \n",
    "doc_vectors = [array(build_vector(tl, voc)) for tl in token_lists]\n",
    "\n",
    "# Step 4. Do the clustering. Feel free to pick between K-Means and GAAC. \n",
    "# How many clusters might you want to generate?\n",
    "\n",
    "num_clusters = 4\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=5)\n",
    "clusters = kclusterer.cluster(doc_vectors, True) \n",
    "\n",
    "doc_clusters = zip(inaugural.fileids(), clusters)\n",
    "for dc in doc_clusters:\n",
    "    pprint(dc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
